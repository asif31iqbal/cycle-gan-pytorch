{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as ff\n",
    "import torchvision.datasets as dsets\n",
    "from PIL import Image\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_general(input_dim, output_dim, kernel_size, stride, padding=0,\n",
    "                 norm=nn.InstanceNorm2d, normalize=True, activate=True, relu_factor=0):\n",
    "    ops = list()\n",
    "    ops.append(nn.Conv2d(input_dim, output_dim, kernel_size, stride, padding, bias=False))\n",
    "    \n",
    "    if normalize:\n",
    "        ops.append(norm(output_dim))\n",
    "    \n",
    "    if activate:\n",
    "        if relu_factor:\n",
    "            relu = nn.LeakyReLU(relu_factor)\n",
    "        else:\n",
    "            relu = nn.ReLU()\n",
    "        ops.append(relu)\n",
    "        \n",
    "    return nn.Sequential(*ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv_general(input_dim, output_dim, kernel_size, stride, padding=0, output_padding=0,\n",
    "                   norm=nn.InstanceNorm2d, normalize=True, activate=True, relu_factor=0):\n",
    "    ops = list()\n",
    "    ops.append(nn.ConvTranspose2d(input_dim, output_dim, kernel_size, stride,\n",
    "                                  padding, output_padding, bias=False))\n",
    "    \n",
    "    if normalize:\n",
    "        ops.append(norm(output_dim))\n",
    "    \n",
    "    if activate:\n",
    "        if relu_factor:\n",
    "            relu = nn.LeakyReLU(relu_factor)\n",
    "        else:\n",
    "            relu = nn.ReLU()\n",
    "        ops.append(relu)\n",
    "        \n",
    "    return nn.Sequential(*ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.refl_pad = nn.ReflectionPad2d(1)\n",
    "        self.conv_general = conv_general(input_dim, output_dim, 3, 1)\n",
    "        self.conv = nn.Conv2d(output_dim, output_dim, 3, 1)\n",
    "        self.instance_norm = nn.InstanceNorm2d(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.refl_pad(x)\n",
    "        o = self.conv_general(x)\n",
    "        o = self.refl_pad(x)\n",
    "        o = self.conv(x)\n",
    "        o = self.instance_norm(x)\n",
    "        \n",
    "        return x + o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, channels=64, residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # 3 input image channels, 2566 output channels, 7*7 square convolution\n",
    "        # kernel\n",
    "        self.residual_blocks = residual_blocks\n",
    "        self.refl_pad = nn.ReflectionPad2d(3)\n",
    "        \n",
    "        self.conv_general1 = conv_general(3, channels, 7, 1)\n",
    "        self.conv_general2 = conv_general(channels, channels * 2, 3, 2, 1)\n",
    "        self.conv_general3 = conv_general(channels * 2, channels * 4, 3, 2, 1)\n",
    "        \n",
    "        self.res_block = ResidualBlock(channels * 4, channels * 4)\n",
    "        \n",
    "        self.deconv_general1 = deconv_general(channels * 4, channels * 2, 3, 2, 1, 1)\n",
    "        self.deconv_general2 = deconv_general(channels * 2, channels, 3, 2, 1, 1)\n",
    "        \n",
    "        self.conv = nn.Conv2d(channels, 3, 7, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = self.refl_pad(x)\n",
    "        x = self.conv_general1(x)\n",
    "        x = self.conv_general2(x)\n",
    "        x = self.conv_general3(x)\n",
    "        \n",
    "        # transformer\n",
    "        for i in range(self.residual_blocks):\n",
    "            x = self.res_block(x)\n",
    "        \n",
    "        # decoder\n",
    "        x = self.deconv_general1(x)\n",
    "        x = self.deconv_general2(x)        \n",
    "        x = self.refl_pad(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, channels=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # 3 input image channels, 2566 output channels, 7*7 square convolution\n",
    "        # kernel\n",
    "        \n",
    "        self.conv_general1 = conv_general(3, channels, 4, 2, 1, normalize=False, relu_factor=0.02)\n",
    "        self.conv_general2 = conv_general(channels, channels * 2, 4, 2, 1)\n",
    "        self.conv_general3 = conv_general(channels * 2, channels * 4, 4, 2, 1)\n",
    "        self.conv_general4 = conv_general(channels * 4, channels * 8, 4, 1, 1)       \n",
    "        self.conv = nn.Conv2d(channels * 8, 1, 4, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_general1(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_general2(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_general3(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_general4(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(3 * 256 * 256).reshape(1, 3, 256, 256).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "gen = Generator()\n",
    "o = gen(a)\n",
    "dis = Discriminator()\n",
    "o = dis(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_x_to_y = Generator()\n",
    "gen_y_to_x = Generator()\n",
    "disc_x = Discriminator()\n",
    "disc_y = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "def cuda_devices(gpu_ids):\n",
    "    gpu_ids = [str(i) for i in gpu_ids]\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpu_ids)\n",
    "\n",
    "\n",
    "def cuda(xs):\n",
    "    if torch.cuda.is_available():\n",
    "        if not isinstance(xs, (list, tuple)):\n",
    "            return xs.cuda()\n",
    "        else:\n",
    "            return [x.cuda() for x in xs]\n",
    "\n",
    "\n",
    "def save_checkpoint(state, save_path, is_best=False, max_keep=None):\n",
    "    # save checkpoint\n",
    "    torch.save(state, save_path)\n",
    "\n",
    "    # deal with max_keep\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    list_path = os.path.join(save_dir, 'latest_checkpoint')\n",
    "\n",
    "    save_path = os.path.basename(save_path)\n",
    "    if os.path.exists(list_path):\n",
    "        with open(list_path) as f:\n",
    "            ckpt_list = f.readlines()\n",
    "            ckpt_list = [save_path + '\\n'] + ckpt_list\n",
    "    else:\n",
    "        ckpt_list = [save_path + '\\n']\n",
    "\n",
    "    if max_keep is not None:\n",
    "        for ckpt in ckpt_list[max_keep:]:\n",
    "            ckpt = os.path.join(save_dir, ckpt[:-1])\n",
    "            if os.path.exists(ckpt):\n",
    "                os.remove(ckpt)\n",
    "        ckpt_list[max_keep:] = []\n",
    "\n",
    "    with open(list_path, 'w') as f:\n",
    "        f.writelines(ckpt_list)\n",
    "\n",
    "    # copy best\n",
    "    if is_best:\n",
    "        shutil.copyfile(save_path, os.path.join(save_dir, 'best_model.ckpt'))\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_dir_or_file, map_location=None, load_best=False):\n",
    "    if os.path.isdir(ckpt_dir_or_file):\n",
    "        if load_best:\n",
    "            ckpt_path = os.path.join(ckpt_dir_or_file, 'best_model.ckpt')\n",
    "        else:\n",
    "            with open(os.path.join(ckpt_dir_or_file, 'latest_checkpoint')) as f:\n",
    "                ckpt_path = os.path.join(ckpt_dir_or_file, f.readline()[:-1])\n",
    "    else:\n",
    "        ckpt_path = ckpt_dir_or_file\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    print(' [*] Loading checkpoint from %s succeed!' % ckpt_path)\n",
    "    return ckpt\n",
    "\n",
    "\n",
    "def reorganize(dataset_dir):\n",
    "    dirs = {}\n",
    "    dirs['trainA'] = os.path.join(dataset_dir, 'link_trainA')\n",
    "    dirs['trainB'] = os.path.join(dataset_dir, 'link_trainB')\n",
    "    dirs['testA'] = os.path.join(dataset_dir, 'link_testA')\n",
    "    dirs['testB'] = os.path.join(dataset_dir, 'link_testB')\n",
    "    mkdir(list(dirs.values()))\n",
    "\n",
    "    for key in dirs:\n",
    "        try:\n",
    "            os.remove(os.path.join(dirs[key], '0'))\n",
    "        except:\n",
    "            pass\n",
    "        os.symlink(os.path.abspath(os.path.join(dataset_dir, key)),\n",
    "                   os.path.join(dirs[key], '0'))\n",
    "\n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemPool(object):\n",
    "\n",
    "    def __init__(self, max_num=50):\n",
    "        self.max_num = max_num\n",
    "        self.num = 0\n",
    "        self.items = []\n",
    "\n",
    "    def __call__(self, in_items):\n",
    "        \"\"\"`in_items` is a list of item.\"\"\"\n",
    "        if self.max_num <= 0:\n",
    "            return in_items\n",
    "        return_items = []\n",
    "        for in_item in in_items:\n",
    "            if self.num < self.max_num:\n",
    "                self.items.append(in_item)\n",
    "                self.num = self.num + 1\n",
    "                return_items.append(in_item)\n",
    "            else:\n",
    "                if np.random.ranf() > 0.5:\n",
    "                    idx = np.random.randint(0, self.max_num)\n",
    "                    tmp = copy.copy(self.items[idx])\n",
    "                    self.items[idx] = in_item\n",
    "                    return_items.append(tmp)\n",
    "                else:\n",
    "                    return_items.append(in_item)\n",
    "        return return_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 1\n",
    "lr = 0.0002\n",
    "dataset_dir = 'datasets/horse2zebra'\n",
    "\n",
    "load_size = 286\n",
    "crop_size = 256\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.Resize(load_size),\n",
    "     transforms.RandomCrop(crop_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)])\n",
    "\n",
    "dataset_dirs = reorganize(dataset_dir)\n",
    "a_data = dsets.ImageFolder(dataset_dirs['trainA'], transform=transform)\n",
    "b_data = dsets.ImageFolder(dataset_dirs['trainB'], transform=transform)\n",
    "a_test_data = dsets.ImageFolder(dataset_dirs['testA'], transform=transform)\n",
    "b_test_data = dsets.ImageFolder(dataset_dirs['testB'], transform=transform)\n",
    "a_loader = torch.utils.data.DataLoader(a_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "b_loader = torch.utils.data.DataLoader(b_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "a_test_loader = torch.utils.data.DataLoader(a_test_data, batch_size=3, shuffle=True, num_workers=4)\n",
    "b_test_loader = torch.utils.data.DataLoader(b_test_data, batch_size=3, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "Da = Discriminator()\n",
    "Db = Discriminator()\n",
    "Ga = Generator()\n",
    "Gb = Generator()\n",
    "MSE = nn.MSELoss()\n",
    "L1 = nn.L1Loss()\n",
    "utils.cuda([Da, Db, Ga, Gb])\n",
    "\n",
    "da_optimizer = torch.optim.Adam(Da.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "db_optimizer = torch.optim.Adam(Db.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "ga_optimizer = torch.optim.Adam(Ga.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "gb_optimizer = torch.optim.Adam(Gb.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n",
      "torch.Size([2, 3, 256, 256])\n",
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "a_fake_pool = ItemPool()\n",
    "for i, ((a_real, a_label), (b_real, b_label)) in enumerate(zip(a_loader, b_loader)):\n",
    "    a_real = torch.autograd.Variable(a_real)\n",
    "    b_real = torch.autograd.Variable(b_real)\n",
    "    print(b_real.shape)\n",
    "    a_fake = gen(b_real)\n",
    "    x = a_fake_pool([a_fake])\n",
    "    a_fake = torch.autograd.Variable(torch.Tensor(x[0]))\n",
    "    print(a_fake.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 3, 256, 256).float()\n",
    "y = torch.rand(1, 3, 256, 256).float()\n",
    "torch.cat([x, y, x, y], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.utils.save_image(torch.cat([x, y, x, y], dim=0), 'gheu.jpg', nrow=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
