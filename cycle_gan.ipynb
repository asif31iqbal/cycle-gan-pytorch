{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as ff\n",
    "import torchvision.datasets as dsets\n",
    "from PIL import Image\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_general(input_dim, output_dim, kernel_size, stride, padding=0,\n",
    "                 norm=nn.InstanceNorm2d, normalize=True, activate=True, relu_factor=0):\n",
    "    ops = list()\n",
    "    ops.append(nn.Conv2d(input_dim, output_dim, kernel_size, stride, padding, bias=False))\n",
    "    \n",
    "    if normalize:\n",
    "        ops.append(norm(output_dim))\n",
    "    \n",
    "    if activate:\n",
    "        if relu_factor:\n",
    "            relu = nn.LeakyReLU(relu_factor)\n",
    "        else:\n",
    "            relu = nn.ReLU()\n",
    "        ops.append(relu)\n",
    "        \n",
    "    return nn.Sequential(*ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv_general(input_dim, output_dim, kernel_size, stride, padding=0, output_padding=0,\n",
    "                   norm=nn.InstanceNorm2d, normalize=True, activate=True, relu_factor=0):\n",
    "    ops = list()\n",
    "    ops.append(nn.ConvTranspose2d(input_dim, output_dim, kernel_size, stride,\n",
    "                                  padding, output_padding, bias=False))\n",
    "    \n",
    "    if normalize:\n",
    "        ops.append(norm(output_dim))\n",
    "    \n",
    "    if activate:\n",
    "        if relu_factor:\n",
    "            relu = nn.LeakyReLU(relu_factor)\n",
    "        else:\n",
    "            relu = nn.ReLU()\n",
    "        ops.append(relu)\n",
    "        \n",
    "    return nn.Sequential(*ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.refl_pad = nn.ReflectionPad2d(1)\n",
    "        self.conv_general = conv_general(input_dim, output_dim, 3, 1)\n",
    "        self.conv = nn.Conv2d(output_dim, output_dim, 3, 1)\n",
    "        self.instance_norm = nn.InstanceNorm2d(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.refl_pad(x)\n",
    "        o = self.conv_general(x)\n",
    "        o = self.refl_pad(x)\n",
    "        o = self.conv(x)\n",
    "        o = self.instance_norm(x)\n",
    "        \n",
    "        return x + o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, channels=64, residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # 3 input image channels, 2566 output channels, 7*7 square convolution\n",
    "        # kernel\n",
    "        self.residual_blocks = residual_blocks\n",
    "        self.refl_pad = nn.ReflectionPad2d(3)\n",
    "        \n",
    "        self.conv_general1 = conv_general(3, channels, 7, 1)\n",
    "        self.conv_general2 = conv_general(channels, channels * 2, 3, 2, 1)\n",
    "        self.conv_general3 = conv_general(channels * 2, channels * 4, 3, 2, 1)\n",
    "        \n",
    "        self.res_block = ResidualBlock(channels * 4, channels * 4)\n",
    "        \n",
    "        self.deconv_general1 = deconv_general(channels * 4, channels * 2, 3, 2, 1, 1)\n",
    "        self.deconv_general2 = deconv_general(channels * 2, channels, 3, 2, 1, 1)\n",
    "        \n",
    "        self.conv = nn.Conv2d(channels, 3, 7, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = self.refl_pad(x)\n",
    "        x = self.conv_general1(x)\n",
    "        x = self.conv_general2(x)\n",
    "        x = self.conv_general3(x)\n",
    "        \n",
    "        # transformer\n",
    "        for i in range(self.residual_blocks):\n",
    "            x = self.res_block(x)\n",
    "        \n",
    "        # decoder\n",
    "        x = self.deconv_general1(x)\n",
    "        x = self.deconv_general2(x)        \n",
    "        x = self.refl_pad(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, channels=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # 3 input image channels, 2566 output channels, 7*7 square convolution\n",
    "        # kernel\n",
    "        \n",
    "        self.conv_general1 = conv_general(3, channels, 4, 2, 1, normalize=False, relu_factor=0.02)\n",
    "        self.conv_general2 = conv_general(channels, channels * 2, 4, 2, 1)\n",
    "        self.conv_general3 = conv_general(channels * 2, channels * 4, 4, 2, 1)\n",
    "        self.conv_general4 = conv_general(channels * 4, channels * 8, 4, 1, 1)       \n",
    "        self.conv = nn.Conv2d(channels * 8, 1, 4, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_general1(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_general2(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_general3(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_general4(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(3 * 256 * 256).reshape(1, 3, 256, 256).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "gen = Generator()\n",
    "o = gen(a)\n",
    "dis = Discriminator()\n",
    "o = dis(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_x_to_y = Generator()\n",
    "gen_y_to_x = Generator()\n",
    "disc_x = Discriminator()\n",
    "disc_y = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def mkdir(paths):\n",
    "    if not isinstance(paths, (list, tuple)):\n",
    "        paths = [paths]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "def cuda_devices(gpu_ids):\n",
    "    gpu_ids = [str(i) for i in gpu_ids]\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpu_ids)\n",
    "\n",
    "\n",
    "def cuda(xs):\n",
    "    if torch.cuda.is_available():\n",
    "        if not isinstance(xs, (list, tuple)):\n",
    "            return xs.cuda()\n",
    "        else:\n",
    "            return [x.cuda() for x in xs]\n",
    "    else:\n",
    "        return xs\n",
    "\n",
    "\n",
    "def save_checkpoint(state, save_path, is_best=False, max_keep=None):\n",
    "    # save checkpoint\n",
    "    torch.save(state, save_path)\n",
    "\n",
    "    # deal with max_keep\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    list_path = os.path.join(save_dir, 'latest_checkpoint')\n",
    "\n",
    "    save_path = os.path.basename(save_path)\n",
    "    if os.path.exists(list_path):\n",
    "        with open(list_path) as f:\n",
    "            ckpt_list = f.readlines()\n",
    "            ckpt_list = [save_path + '\\n'] + ckpt_list\n",
    "    else:\n",
    "        ckpt_list = [save_path + '\\n']\n",
    "\n",
    "    if max_keep is not None:\n",
    "        for ckpt in ckpt_list[max_keep:]:\n",
    "            ckpt = os.path.join(save_dir, ckpt[:-1])\n",
    "            if os.path.exists(ckpt):\n",
    "                os.remove(ckpt)\n",
    "        ckpt_list[max_keep:] = []\n",
    "\n",
    "    with open(list_path, 'w') as f:\n",
    "        f.writelines(ckpt_list)\n",
    "\n",
    "    # copy best\n",
    "    if is_best:\n",
    "        shutil.copyfile(save_path, os.path.join(save_dir, 'best_model.ckpt'))\n",
    "\n",
    "\n",
    "def load_checkpoint(ckpt_dir_or_file, map_location=None, load_best=False):\n",
    "    if os.path.isdir(ckpt_dir_or_file):\n",
    "        if load_best:\n",
    "            ckpt_path = os.path.join(ckpt_dir_or_file, 'best_model.ckpt')\n",
    "        else:\n",
    "            with open(os.path.join(ckpt_dir_or_file, 'latest_checkpoint')) as f:\n",
    "                ckpt_path = os.path.join(ckpt_dir_or_file, f.readline()[:-1])\n",
    "    else:\n",
    "        ckpt_path = ckpt_dir_or_file\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    print(' [*] Loading checkpoint from %s succeed!' % ckpt_path)\n",
    "    return ckpt\n",
    "\n",
    "\n",
    "def reorganize(dataset_dir):\n",
    "    dirs = {}\n",
    "    dirs['trainA'] = os.path.join(dataset_dir, 'link_trainA')\n",
    "    dirs['trainB'] = os.path.join(dataset_dir, 'link_trainB')\n",
    "    dirs['testA'] = os.path.join(dataset_dir, 'link_testA')\n",
    "    dirs['testB'] = os.path.join(dataset_dir, 'link_testB')\n",
    "    mkdir(list(dirs.values()))\n",
    "\n",
    "    for key in dirs:\n",
    "        try:\n",
    "            os.remove(os.path.join(dirs[key], '0'))\n",
    "        except:\n",
    "            pass\n",
    "        os.symlink(os.path.abspath(os.path.join(dataset_dir, key)),\n",
    "                   os.path.join(dirs[key], '0'))\n",
    "\n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemPool(object):\n",
    "\n",
    "    def __init__(self, max_num=50):\n",
    "        self.max_num = max_num\n",
    "        self.num = 0\n",
    "        self.items = []\n",
    "\n",
    "    def __call__(self, in_items):\n",
    "        \"\"\"`in_items` is a list of item.\"\"\"\n",
    "        if self.max_num <= 0:\n",
    "            return in_items\n",
    "        return_items = []\n",
    "        for in_item in in_items:\n",
    "            if self.num < self.max_num:\n",
    "                self.items.append(in_item)\n",
    "                self.num = self.num + 1\n",
    "                return_items.append(in_item)\n",
    "            else:\n",
    "                if np.random.ranf() > 0.5:\n",
    "                    idx = np.random.randint(0, self.max_num)\n",
    "                    tmp = copy.copy(self.items[idx])\n",
    "                    self.items[idx] = in_item\n",
    "                    return_items.append(tmp)\n",
    "                else:\n",
    "                    return_items.append(in_item)\n",
    "        return return_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "start_epoch = 0\n",
    "batch_size = 1\n",
    "lr = 0.0002\n",
    "dataset_dir = 'datasets/horse2zebra'\n",
    "\n",
    "load_size = 286\n",
    "crop_size = 256\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.Resize(load_size),\n",
    "     transforms.RandomCrop(crop_size),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)])\n",
    "\n",
    "dataset_dirs = reorganize(dataset_dir)\n",
    "a_train_data = dsets.ImageFolder(dataset_dirs['trainA'], transform=transform)\n",
    "b_train_data = dsets.ImageFolder(dataset_dirs['trainB'], transform=transform)\n",
    "a_test_data = dsets.ImageFolder(dataset_dirs['testA'], transform=transform)\n",
    "b_test_data = dsets.ImageFolder(dataset_dirs['testB'], transform=transform)\n",
    "a_train_loader = torch.utils.data.DataLoader(a_train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "b_train_loader = torch.utils.data.DataLoader(b_train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "a_test_loader = torch.utils.data.DataLoader(a_test_data, batch_size=3, shuffle=True, num_workers=4)\n",
    "b_test_loader = torch.utils.data.DataLoader(b_test_data, batch_size=3, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "disc_a = Discriminator()\n",
    "disc_b = Discriminator()\n",
    "gen_a = Generator()\n",
    "gen_b = Generator()\n",
    "MSE = nn.MSELoss()\n",
    "L1 = nn.L1Loss()\n",
    "cuda([disc_a, disc_b, gen_a, gen_b])\n",
    "\n",
    "disc_a_optimizer = torch.optim.Adam(disc_a.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "disc_b_optimizer = torch.optim.Adam(disc_b.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "gen_a_optimizer = torch.optim.Adam(gen_a.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "gen_b_optimizer = torch.optim.Adam(gen_b.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "a_fake_pool = ItemPool()\n",
    "b_fake_pool = ItemPool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a_test_real = torch.autograd.Variable(iter(a_test_loader).next()[0])\n",
    "    b_test_real = torch.autograd.Variable(iter(b_test_loader).next()[0])\n",
    "a_test_real, b_test_real = cuda([a_test_real, b_test_real])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "Epoch: (  0) (    1/ 1067)\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([1, 128, 64, 64])\n",
      "torch.Size([1, 256, 32, 32])\n",
      "torch.Size([1, 512, 31, 31])\n",
      "torch.Size([1, 1, 30, 30])\n",
      "Epoch: (  1) (    1/ 1067)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(start_epoch, 2):\n",
    "    for i, ((a_train_real, _), (b_train_real, _)) in enumerate(zip(a_train_loader, b_train_loader)):\n",
    "        step = epoch * min(len(a_loader), len(b_loader)) + i + 1\n",
    "        \n",
    "        gen_a.train()\n",
    "        gen_b.train()\n",
    "        \n",
    "#         a_train_real.requires_grad = True\n",
    "#         b_train_real.requires_grad = True\n",
    "        a_train_real, b_train_real = cuda([a_train_real, b_train_real])\n",
    "    \n",
    "        # generate fake images\n",
    "        a_train_fake = gen_a(b_train_real)\n",
    "        b_train_fake = gen_b(a_train_real)\n",
    "        \n",
    "        a_train_cycle = gen_a(b_train_fake)\n",
    "        b_train_cycle = gen_b(a_train_fake)\n",
    "        \n",
    "        \n",
    "        a_train_fake_disc = disc_a(a_train_fake)\n",
    "        b_train_fake_disc = disc_b(b_train_fake)\n",
    "        \n",
    "        # generator loss\n",
    "        real_label = cuda(torch.ones(a_train_fake_disc.size()))\n",
    "        a_train_loss_gen = MSE(a_train_fake_disc, real_label)\n",
    "        b_train_loss_gen = MSE(b_train_fake_disc, real_label)\n",
    "        \n",
    "        # cyclic loss\n",
    "        a_train_loss_cycle = L1(a_train_cycle, a_train_real)\n",
    "        b_train_loss_cycle = L1(b_train_cycle, b_train_real)\n",
    "        \n",
    "        train_loss_gen = a_train_loss_gen + b_train_loss_gen + 10.0 * (a_train_loss_cycle + b_train_loss_cycle)\n",
    "        \n",
    "        # generator backprop\n",
    "        gen_a.zero_grad()\n",
    "        gen_b.zero_grad()\n",
    "        train_loss_gen.backward()\n",
    "        gen_a_optimizer.step()\n",
    "        gen_b_optimizer.step()\n",
    "        \n",
    "        a_train_fake = torch.Tensor(a_fake_pool([a_train_fake.detach().numpy()])[0])\n",
    "        b_train_fake = torch.Tensor(b_fake_pool([b_train_fake.detach().numpy()])[0])\n",
    "        a_train_fake, b_train_fake = cuda([a_train_fake, b_train_fake])\n",
    "        \n",
    "        \n",
    "        # train discriminators\n",
    "        a_train_real_disc = disc_a(a_train_real)\n",
    "        a_train_fake_disc = disc_a(a_train_fake)\n",
    "        b_train_real_disc = disc_b(b_train_real)\n",
    "        b_train_fake_disc = disc_b(b_train_fake)\n",
    "        real_label = cuda(torch.ones(a_train_fake_disc.size()))\n",
    "        fake_label = cuda(torch.zeros(a_train_fake_disc.size()))\n",
    "        \n",
    "        # discriminator loss\n",
    "        a_train_real_loss_disc = MSE(a_train_real_disc, real_label)\n",
    "        a_train_fake_loss_disc = MSE(a_train_fake_disc, fake_label)\n",
    "        b_train_real_loss_disc = MSE(b_train_real_disc, real_label)\n",
    "        b_train_fake_loss_disc = MSE(b_train_fake_disc, fake_label)\n",
    "\n",
    "        a_train_loss_disc = a_train_real_loss_disc + a_train_fake_loss_disc\n",
    "        b_train_loss_disc = b_train_real_loss_disc + b_train_fake_loss_disc\n",
    "        \n",
    "        # discriminator backprop\n",
    "        disc_a.zero_grad()\n",
    "        disc_b.zero_grad()\n",
    "        a_train_loss_disc.backward()\n",
    "        b_train_loss_disc.backward()\n",
    "        disc_a_optimizer.step()\n",
    "        disc_b_optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 1 == 0:\n",
    "            print(\"Epoch: (%3d) (%5d/%5d)\" % (epoch, i + 1, min(len(a_loader), len(b_loader))))\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            gen_a.eval()\n",
    "            gen_b.eval()\n",
    "\n",
    "            # train G\n",
    "            a_test_fake = gen_a(b_test_real)\n",
    "            b_test_fake = gen_b(a_test_real)\n",
    "\n",
    "            a_test_cycle = gen_a(b_test_fake)\n",
    "            b_test_cycle = gen_b(a_test_fake)\n",
    "\n",
    "            pic = torch.cat([a_test_real, b_test_fake, a_test_cycle, \n",
    "                              b_test_real, a_test_fake, b_test_cycle], \n",
    "                             dim=0).data / 2.0 + 0.5\n",
    "\n",
    "            save_dir = './sample_images_while_training'\n",
    "            utils.mkdir(save_dir)\n",
    "            torchvision.utils.save_image(pic, '%s/Epoch_(%d)_(%dof%d).jpg' % (save_dir, epoch, i + 1, min(len(a_loader), len(b_loader))), nrow=3)\n",
    "\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.2638e-01, -3.6851e-01,  2.3288e-01,  ...,  4.0371e-01,\n",
       "            4.6333e-01,  2.3077e-01],\n",
       "          [ 3.0675e-02,  4.4485e-01,  2.8081e-01,  ...,  2.0270e-01,\n",
       "            1.4395e-01,  4.9138e-01],\n",
       "          [-2.4351e-01,  5.0338e-01,  1.5834e-01,  ...,  2.8667e-01,\n",
       "            1.5118e-01,  5.1702e-01],\n",
       "          ...,\n",
       "          [-3.1655e-01,  2.7835e-01, -1.8281e-02,  ...,  2.7307e-01,\n",
       "            1.8649e-01,  5.5607e-03],\n",
       "          [ 9.4958e-02,  3.2726e-01, -5.6268e-02,  ...,  1.7509e-01,\n",
       "           -1.1852e-02,  1.3652e-01],\n",
       "          [-1.0770e-01,  1.9923e-01,  4.0159e-02,  ...,  9.6832e-02,\n",
       "           -5.9244e-02, -1.0726e-01]],\n",
       "\n",
       "         [[-3.1499e-01,  5.8623e-01,  6.2428e-02,  ...,  1.8101e-01,\n",
       "            1.1826e-01,  2.8713e-02],\n",
       "          [-2.2162e-01, -2.0546e-01,  8.8168e-02,  ...,  3.2158e-01,\n",
       "            2.6489e-01, -5.0689e-01],\n",
       "          [-1.9654e-01,  2.4779e-01, -1.5417e-01,  ..., -4.3926e-02,\n",
       "            3.8946e-01, -2.6373e-02],\n",
       "          ...,\n",
       "          [-6.8771e-02,  1.6693e-01, -9.1351e-02,  ..., -1.2621e-01,\n",
       "            2.7861e-02,  1.0039e-01],\n",
       "          [ 2.5740e-01, -1.4191e-01,  9.2145e-02,  ...,  7.7593e-02,\n",
       "            6.9897e-02,  1.9428e-01],\n",
       "          [-1.9316e-01, -7.3683e-03,  6.3507e-02,  ..., -1.3459e-02,\n",
       "           -9.2100e-03,  1.1281e-01]],\n",
       "\n",
       "         [[-1.0374e-01,  2.9941e-01, -1.9884e-01,  ...,  4.8508e-01,\n",
       "           -1.3163e-01, -2.7580e-02],\n",
       "          [ 2.0307e-01,  9.6292e-02,  2.7833e-01,  ..., -2.5315e-01,\n",
       "            2.7635e-01, -4.5096e-03],\n",
       "          [ 3.6355e-01,  3.3127e-01,  5.3289e-01,  ...,  3.9481e-01,\n",
       "           -9.8735e-02, -2.3169e-01],\n",
       "          ...,\n",
       "          [-2.7335e-01, -1.6986e-02, -2.6654e-01,  ..., -6.7057e-02,\n",
       "           -2.7173e-01,  1.6952e-02],\n",
       "          [ 1.2057e-01, -3.0747e-01,  1.3863e-01,  ..., -1.1226e-01,\n",
       "            2.8813e-01, -2.2889e-01],\n",
       "          [ 7.1949e-02, -2.6975e-02, -2.8698e-02,  ..., -2.1593e-02,\n",
       "           -8.4743e-02, -5.6715e-02]]]])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n",
      "torch.Size([2, 3, 256, 256])\n",
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "a_fake_pool = ItemPool()\n",
    "for i, ((a_real, a_label), (b_real, b_label)) in enumerate(zip(a_loader, b_loader)):\n",
    "    a_real = torch.autograd.Variable(a_real)\n",
    "    b_real = torch.autograd.Variable(b_real)\n",
    "    print(b_real.shape)\n",
    "    a_fake = gen(b_real)\n",
    "    x = a_fake_pool([a_fake])\n",
    "    a_fake = torch.autograd.Variable(torch.Tensor(x[0]))\n",
    "    print(a_fake.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 3, 256, 256).float()\n",
    "y = torch.rand(1, 3, 256, 256).float()\n",
    "torch.cat([x, y, x, y], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.utils.save_image(torch.cat([x, y, x, y], dim=0), 'gheu.jpg', nrow=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
